{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './stanford-car-dataset-by-classes-folder/car_data/'\n",
    "train_dir = data_dir + 'train'\n",
    "test_dir = data_dir + 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([transforms.RandomRotation(30), \n",
    "                                          transforms.RandomResizedCrop(224), \n",
    "                                          transforms.RandomHorizontalFlip(), \n",
    "                                          transforms.ToTensor()])\n",
    "validation_tranforms = transforms.Compose([transforms.Resize(256), \n",
    "                                         transforms.CenterCrop(224), \n",
    "                                         transforms.ToTensor()])\n",
    "testing_transforms = transforms.Compose([transforms.Resize(256), \n",
    "                                         transforms.CenterCrop(224), \n",
    "                                         transforms.ToTensor()])\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=training_transforms)\n",
    "validation_data = datasets.ImageFolder(train_dir, transform=validation_tranforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=testing_transforms)\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.2 * num_train))\n",
    "np.random.shuffle(indices)\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "train_idx, validation_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=64)\n",
    "validate_loader = torch.utils.data.DataLoader(validation_data,\n",
    "                   sampler=validation_sampler, batch_size=64)\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (fc1): Linear(in_features=25088, out_features=5000, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (drop): Dropout(p=0.5)\n",
       "    (fc2): Linear(in_features=5000, out_features=196, bias=True)\n",
       "    (output): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "from collections import OrderedDict\n",
    "    \n",
    "model.classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 5000)), \n",
    "                                              ('relu', nn.ReLU()), \n",
    "                                              ('drop', nn.Dropout(0.5)), \n",
    "                                              ('fc2', nn.Linear(5000, 196)), \n",
    "                                              ('output', nn.LogSoftmax(dim=1))]))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6ea60b07f479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidate_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mlogps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[1;32m    131\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transparency'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \"\"\"\n\u001b[1;32m   1108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0m__copy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 10\n",
    "train_losses, validation_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(len(train_loader))\n",
    "    for inputs, labels in train_loader:\n",
    "        steps += 1\n",
    "        print(steps)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            val_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                print(len(validate_loader))\n",
    "                for inputs, labels in validate_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    val_loss += batch_loss.item()\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            validation_losses.append(val_loss/len(validate_loader))                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {val_loss/len(validate_loader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(validate_loader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()\n",
    "            \n",
    "torch.save(model, 'aerialmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(validation_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
